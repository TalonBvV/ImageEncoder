{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Multi-Task Image Encoder Tutorial\n",
    "\n",
    "This notebook provides a complete walkthrough of how to install, train, and use the multi-task image encoder."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 1: Clone the Repository\n",
    "\n",
    "First, we'll clone the project repository from GitHub. **Remember to replace the URL with your actual repository URL.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!git clone https://github.com/TalonBvV/ImageEncoder.git image-encoder\n",
    "%cd image-encoder"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 2: Install Dependencies\n",
    "\n",
    "Now, we'll install the project and all its dependencies using the `setup.py` and `requirements.txt` files."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install ."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 3: Download a Sample Dataset\n",
    "\n",
    "We need some images to train on. We'll download a small set of flower photos as a demonstration."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!wget -q -O flower_photos.tgz http://download.tensorflow.org/example_images/flower_photos.tgz\n",
    "!tar -xzf flower_photos.tgz\n",
    "IMAGE_DIR = 'flower_photos'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 4: Configure and Run Training\n",
    "\n",
    "We need to tell our training script where to find the images. We'll modify the `train.py` file to point to our downloaded dataset and reduce the number of epochs for a quick demonstration."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Modify the train.py script to use our new image directory and run for just 2 epochs\n",
    "!sed -i \"s|IMAGE_DIR = .*|IMAGE_DIR = 'flower_photos'|g\" train.py\n",
    "!sed -i \"s|MAX_EPOCHS = .*|MAX_EPOCHS = 2|g\" train.py\n",
    "\n",
    "print('--- Modified train.py ---')\n",
    "!cat train.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run the training!\n",
    "!python train.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 5: Use the Trained Encoder for Inference\n",
    "\n",
    "After training, a checkpoint file is saved. We can now load this checkpoint, extract the trained encoder, and use it to get a latent vector for a new image."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from PIL import Image\n",
    "from torchvision import transforms\n",
    "from lightning_module import MultiTaskImageEncoder\n",
    "import glob\n",
    "\n",
    "# Find the checkpoint file\n",
    "checkpoint_path = glob.glob('tb_logs/image_encoder_v1/version_0/checkpoints/*.ckpt')[0]\n",
    "print(f'Found checkpoint: {checkpoint_path}')\n",
    "\n",
    "# Load the model from the checkpoint\n",
    "model = MultiTaskImageEncoder.load_from_checkpoint(checkpoint_path)\n",
    "encoder = model.encoder\n",
    "encoder.eval() # Set to evaluation mode\n",
    "\n",
    "# Load a sample image\n",
    "sample_image_path = glob.glob('flower_photos/*/*.jpg')[0]\n",
    "img = Image.open(sample_image_path).convert('RGB')\n",
    "\n",
    "# Preprocess the image\n",
    "transform = transforms.Compose([\n",
    "    transforms.Resize((128, 128)),\n",
    "    transforms.ToTensor(),\n",
    "])\n",
    "img_tensor = transform(img).unsqueeze(0) # Add batch dimension\n",
    "\n",
    "# Get the latent vector\n",
    "with torch.no_grad():\n",
    "    latent_vector = encoder(img_tensor)\n",
    "\n",
    "print(f'Successfully encoded image {sample_image_path}')\n",
    "print(f'Latent vector shape: {latent_vector.shape}')\n",
    "print(f'Latent vector (first 10 values): {latent_vector[0, :10]}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 6: Export the Encoder to ONNX\n",
    "\n",
    "Finally, we'll use the `export.py` script to save the trained encoder to the standard ONNX format for deployment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Modify export.py to use the correct checkpoint path\n",
    "!sed -i \"s|# CHECKPOINT_PATH = .*|CHECKPOINT_PATH = f'{checkpoint_path}'|g\" export.py\n",
    "!sed -i \"s|# export_encoder_to_onnx.*|export_encoder_to_onnx(CHECKPOINT_PATH)|g\" export.py\n",
    "\n",
    "print('--- Modified export.py ---')\n",
    "!cat export.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run the export!\n",
    "!python export.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!ls -l encoder.onnx"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
